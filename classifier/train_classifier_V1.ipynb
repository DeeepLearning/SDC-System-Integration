{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Classifier to detect traffic lights\n",
    "\n",
    "##### Udacity Self-Driving Car Nanodegree\n",
    "##### project system integration\n",
    "##### team vulture\n",
    "\n",
    "This approach follows very closely the one documented in https://github.com/cena0805/ros-traffic-light-classifier as this is almost what we need.\n",
    "\n",
    "Rainer Bareiss, V0.11, August 20th, 2017, Lago Maggiore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from visual_callbacks import AccLossPlotter\n",
    "import model\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epoch = 3 #25\n",
    "nb_train_samples = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 load & explore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# original model from https://github.com/cena0805/ros-traffic-light-classifier\n",
    "model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_25 (Convolution2D) (None, 64, 64, 32)    896         convolution2d_input_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 64, 64, 32)    0           convolution2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_26 (Convolution2D) (None, 62, 62, 32)    9248        activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 62, 62, 32)    0           convolution2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 31, 31, 32)    0           activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 31, 31, 32)    0           maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_27 (Convolution2D) (None, 31, 31, 64)    18496       dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 31, 31, 64)    0           convolution2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_28 (Convolution2D) (None, 29, 29, 64)    36928       activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 29, 29, 64)    0           convolution2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 14, 14, 64)    0           activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 14, 14, 64)    0           maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 12544)         0           dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 512)           6423040     flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 512)           0           dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 512)           0           activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 3)             1539        dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 3)             0           dense_14[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 6,490,147\n",
      "Trainable params: 6,490,147\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: iplement squeezenet and compare accuracy & runtime for small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4 load data and split into training, validate and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set data pathes\n",
    "# input & output folder\n",
    "mydir_in = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/00-team_vulture/SDC-System-Integration/classifier/'\n",
    "mydir_out = '/Users/rainerbareiss/Downloads/traffic_light_images/'\n",
    "#mydir_in = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/00-team_vulture/SDC-System-Integration/classifier'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34972\n",
      "8744\n"
     ]
    }
   ],
   "source": [
    "#mydir = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/90-sample_classifier/model'\n",
    "#myfiles = glob.glob(mydir+\"/images/*/*.*\")\n",
    "\n",
    "myfiles = glob.glob(mydir_in+\"/images/*/*.*\")\n",
    "\n",
    "#print(myfiles)\n",
    "train_samples, validation_samples = train_test_split(myfiles, test_size=0.2)\n",
    "print(len(train_samples))\n",
    "print(len(validation_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5 prepare data fro training \n",
    "\n",
    "### input\n",
    "\n",
    "/images:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown\n",
    "    \n",
    "### output (augmented)\n",
    "\n",
    "/train:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown\n",
    "    \n",
    "/test:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store training data into separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3397157/how-to-read-a-raw-image-using-pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ifile in train_samples:\n",
    "    # read data\n",
    "    file = Image.open(ifile)\n",
    "    imgSize = file.size\n",
    "    rawData = file.tobytes()\n",
    "    \n",
    "    # get class & file name\n",
    "    classdir = ifile.split(\"/\")[-2]   # class name\n",
    "    classname = ifile.split(\"/\")[-1]  # name of the image file\n",
    "    #print(classdir + ' ---> ' + classname)\n",
    "    \n",
    "    # generate new file names\n",
    "    ofile = mydir_out + 'train/' + classdir + '/' + classname\n",
    "    #print(ofile)\n",
    "    \n",
    "    # save in new folder\n",
    "    img = Image.frombytes('RGB', imgSize, rawData)\n",
    "    img.save(ofile)\n",
    "    #print(ifile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34972 images belonging to 3 classes.\n",
      "number of image samples:  34972\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, shear_range=0.05, zoom_range=.1,\n",
    "                             fill_mode='nearest', rescale=1. / 255)\n",
    "train_image_data_gen = datagen.flow_from_directory(mydir_out+'train', target_size=(64, 64), classes=['green', 'red', 'unknown'],\n",
    "                                             batch_size=batch_size)\n",
    "print(\"number of image samples: \",train_image_data_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store test data into separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ground_truth = []\n",
    "\n",
    "for ifile in validation_samples:\n",
    "    # read data\n",
    "    file = Image.open(ifile)\n",
    "    imgSize = file.size\n",
    "    rawData = file.tobytes()\n",
    "    \n",
    "    # get class & file name\n",
    "    classdir = ifile.split(\"/\")[-2]   # class name\n",
    "    classname = ifile.split(\"/\")[-1]  # name of the image file\n",
    "    #print(classdir + ' ---> ' + classname)\n",
    "    \n",
    "    # generate new file names\n",
    "    ofile = mydir_out + 'test/' + classdir + '/' + classname\n",
    "    #print(ofile)\n",
    "    \n",
    "    # save in new folder\n",
    "    img = Image.frombytes('RGB', imgSize, rawData)\n",
    "    img.save(ofile)\n",
    "    #print(ifile)\n",
    "    \n",
    "    # generate vector of ground_truth\n",
    "    if (classdir == 'green'): y_ground_truth.append([1,0,0])\n",
    "    if (classdir == 'red'): y_ground_truth.append([0,1,0])\n",
    "    if (classdir == 'unknown'): y_ground_truth.append([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8744 images belonging to 3 classes.\n",
      "number of image samples:  8744\n"
     ]
    }
   ],
   "source": [
    "test_image_data_gen = datagen.flow_from_directory(mydir_out+'test', target_size=(64, 64), classes=['green', 'red', 'unknown'],\n",
    "                                             batch_size=batch_size)\n",
    "print(\"number of image samples: \",test_image_data_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6 train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/200 [======================================] - 6s - loss: 2.5598 - acc: 0.6055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainerbareiss/Developer/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n",
      "256/200 [======================================] - 5s - loss: 0.9814 - acc: 0.5820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "256/200 [======================================] - 5s - loss: 0.7974 - acc: 0.6602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ce6b128>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_image_data_gen, nb_epoch=nb_epoch, samples_per_epoch=nb_train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('traffic_lights_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_image(image):\n",
    "  x = img_to_array(image.resize((64, 64), Image.ANTIALIAS))\n",
    "  return x[None, :]\n",
    "\n",
    "model_predict = load_model('traffic_lights_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Load Example Image\n",
    "green = load_img(mydir_in + 'images_from_simulator/sampleout1.jpg')\n",
    "\n",
    "# TODO: scale & crop before prediction!!!\n",
    "my_prob = model_predict.predict(reshape_image(green))\n",
    "print(my_prob)\n",
    "print(my_prob[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 analyse performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply prediction over all test data \n",
    "myfiles = glob.glob(mydir_out+\"/test/*/*.*\")\n",
    "my_pred = []\n",
    "\n",
    "for ifile in myfiles:\n",
    "    my_img = load_img(ifile)\n",
    "    my_prob = model_predict.predict(reshape_image(my_img))\n",
    "    my_pred.append([my_prob[0][0],my_prob[0][1],my_prob[0][2]])\n",
    "    #print(max(my_prob))\n",
    "#print(my_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate array of indices showing the class number of prediction\n",
    "my_pred_ind = []\n",
    "\n",
    "for ipred in my_pred:\n",
    "        imax = np.argwhere(ipred == np.amax(ipred))\n",
    "        ##print(ipred, \"     \", imax[0][0])\n",
    "        #my_pred_ind = my_pred_ind.append(imax[0][0])\n",
    "        if (imax[0][0] != 2): my_pred_ind.append(imax[0][0])\n",
    "        \n",
    "#print(my_pred_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate array of indices showing the ground truth class\n",
    "my_true_ind = []\n",
    "\n",
    "for itrue in y_ground_truth:\n",
    "        imax = np.argwhere(ipred == np.amax(ipred))\n",
    "        ##print(ipred, \"     \", imax[0][0])\n",
    "        #my_pred_ind = my_pred_ind.append(imax[0][0])\n",
    "        if (imax[0][0] != 2): my_true_ind.append(imax[0][0])\n",
    "        \n",
    "#print(my_true_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of comparision:  []\n",
      "no red or green lights detected\n"
     ]
    }
   ],
   "source": [
    "# check elementwise for equality and store result in vector\n",
    "comp = np.equal(my_pred_ind, my_true_ind)\n",
    "print(\"array of comparision: \",comp)\n",
    "\n",
    "# count occurences of true\n",
    "collections.Counter(comp)\n",
    "mytrues = Counter(comp)\n",
    "if (len(comp) != 0): \n",
    "    print(\"Accuracy of predicting new signs from the web = \", mytrues[1]/len(comp)*100,\"%\")\n",
    "else:\n",
    "    print(\"no red or green lights detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
