{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Classifier to detect traffic lights\n",
    "\n",
    "##### Udacity Self-Driving Car Nanodegree\n",
    "##### project system integration\n",
    "##### team vulture\n",
    "\n",
    "This approach follows very closely the one documented in https://github.com/cena0805/ros-traffic-light-classifier as this is almost what we need.\n",
    "\n",
    "Rainer Bareiss, V0.1, August 20th, 2017, Lago Maggiore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from visual_callbacks import AccLossPlotter\n",
    "import model\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epoch = 3 #25\n",
    "nb_train_samples = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 load & explore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# original model from https://github.com/cena0805/ros-traffic-light-classifier\n",
    "model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_21 (Convolution2D) (None, 64, 64, 32)    896         convolution2d_input_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 64, 64, 32)    0           convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 62, 62, 32)    9248        activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 62, 62, 32)    0           convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 31, 31, 32)    0           activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 31, 31, 32)    0           maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 31, 31, 64)    18496       dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 31, 31, 64)    0           convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 29, 29, 64)    36928       activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 29, 29, 64)    0           convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 14, 14, 64)    0           activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 14, 14, 64)    0           maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 12544)         0           dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 512)           6423040     flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 512)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 512)           0           activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 3)             1539        dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 3)             0           dense_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 6,490,147\n",
      "Trainable params: 6,490,147\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: iplement squeezenet and compare accuracy & runtime for small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4 load data and split into training, validate and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34972\n",
      "8744\n"
     ]
    }
   ],
   "source": [
    "mydir = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/90-sample_classifier/model'\n",
    "myfiles = glob.glob(mydir+\"/images/*/*.*\")\n",
    "#print(myfiles)\n",
    "train_samples, validation_samples = train_test_split(myfiles, test_size=0.2)\n",
    "print(len(train_samples))\n",
    "print(len(validation_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5 prepare data fro training \n",
    "\n",
    "### input\n",
    "\n",
    "/images:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown\n",
    "    \n",
    "### output (augmented)\n",
    "\n",
    "/train:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown\n",
    "    \n",
    "/valid:\n",
    "    /red\n",
    "    /green\n",
    "    /unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, shear_range=0.05, zoom_range=.1,\n",
    "                             fill_mode='nearest', rescale=1. / 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43716 images belonging to 3 classes.\n",
      "number of image samples:  43716\n",
      "using samples:            200\n"
     ]
    }
   ],
   "source": [
    "image_data_gen = datagen.flow_from_directory('images', target_size=(64, 64), classes=['green', 'red', 'unknown'],\n",
    "                                             batch_size=batch_size)\n",
    "print(\"number of image samples: \",image_data_gen.nb_sample)\n",
    "print(\"using samples:           \",nb_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3397157/how-to-read-a-raw-image-using-pil\n",
    "from PIL import Image as Image\n",
    "\n",
    "mydir_in = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/00-team_vulture/SDC-System-Integration/classifier'\n",
    "# read image\n",
    "i = Image.open(mydir_in+'/images/red/red_123.jpg')\n",
    "imgSize = i.size\n",
    "rawData = i.tobytes()\n",
    "\n",
    "# save in deifferent formats\n",
    "#img = Image.frombytes('L', imgSize, rawData)\n",
    "#img.save('output/lmode.png')\n",
    "img = Image.frombytes('RGB', imgSize, rawData)\n",
    "img.save('output/red_123.jpg')\n",
    "#img = Image.frombytes('RGBX', imgSize, rawData)\n",
    "#img.save('rgbxmode.jfif')\n",
    "#img = Image.frombytes('RGBA', imgSize, rawData)\n",
    "#img.save('rgbamode.png')\n",
    "#img = Image.frombytes('CMYK', imgSize, rawData)\n",
    "#img.save('rgbamode.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store training data into separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3397157/how-to-read-a-raw-image-using-pil\n",
    "\n",
    "# input & output folder\n",
    "mydir_in = '/Users/rainerbareiss/Library/Mobile Documents/M6HJR9W95L~com~textasticapp~textastic/Documents/02-Udacity/carND/03-term3/P3-CarND-Capstone/00-team_vulture/SDC-System-Integration/classifier/'\n",
    "mydir_out = '/Users/rainerbareiss/Downloads/traffic_light_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ifile in train_samples:\n",
    "    # read data\n",
    "    file = Image.open(ifile)\n",
    "    imgSize = file.size\n",
    "    rawData = file.tobytes()\n",
    "    \n",
    "    # get class & file name\n",
    "    classdir = ifile.split(\"/\")[-2]   # class name\n",
    "    classname = ifile.split(\"/\")[-1]  # name of the image file\n",
    "    #print(classdir + ' ---> ' + classname)\n",
    "    \n",
    "    # generate new file names\n",
    "    ofile = mydir_out + 'train/' + classdir + '/' + classname\n",
    "    #print(ofile)\n",
    "    \n",
    "    # save in new folder\n",
    "    img = Image.frombytes('RGB', imgSize, rawData)\n",
    "    img.save(ofile)\n",
    "    #print(ifile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34972 images belonging to 3 classes.\n",
      "number of image samples:  34972\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, shear_range=0.05, zoom_range=.1,\n",
    "                             fill_mode='nearest', rescale=1. / 255)\n",
    "train_image_data_gen = datagen.flow_from_directory(mydir_out+'train', target_size=(64, 64), classes=['green', 'red', 'unknown'],\n",
    "                                             batch_size=batch_size)\n",
    "print(\"number of image samples: \",train_image_data_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store validation data into separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ifile in validation_samples:\n",
    "    # read data\n",
    "    file = Image.open(ifile)\n",
    "    imgSize = file.size\n",
    "    rawData = file.tobytes()\n",
    "    \n",
    "    # get class & file name\n",
    "    classdir = ifile.split(\"/\")[-2]   # class name\n",
    "    classname = ifile.split(\"/\")[-1]  # name of the image file\n",
    "    #print(classdir + ' ---> ' + classname)\n",
    "    \n",
    "    # generate new file names\n",
    "    ofile = mydir_out + 'valid/' + classdir + '/' + classname\n",
    "    #print(ofile)\n",
    "    \n",
    "    # save in new folder\n",
    "    img = Image.frombytes('RGB', imgSize, rawData)\n",
    "    img.save(ofile)\n",
    "    #print(ifile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8744 images belonging to 3 classes.\n",
      "number of image samples:  8744\n"
     ]
    }
   ],
   "source": [
    "valid_image_data_gen = datagen.flow_from_directory(mydir_out+'valid', target_size=(64, 64), classes=['green', 'red', 'unknown'],\n",
    "                                             batch_size=batch_size)\n",
    "print(\"number of image samples: \",valid_image_data_gen.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6 train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/200 [======================================] - 6s - loss: 0.8964 - acc: 0.6172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainerbareiss/Developer/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n",
      "256/200 [======================================] - 5s - loss: 0.7700 - acc: 0.7266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "256/200 [======================================] - 5s - loss: 0.7766 - acc: 0.7227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a4a7748>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_image_data_gen, nb_epoch=nb_epoch, samples_per_epoch=nb_train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('traffic_lights_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def reshape_image(image):\n",
    "  x = img_to_array(image.resize((64, 64), Image.ANTIALIAS))\n",
    "  return x[None, :]\n",
    "\n",
    "model_predict = load_model('traffic_lights_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Load Example Image\n",
    "green = load_img(mydir_in + 'images_from_simulator/sampleout1.jpg')\n",
    "\n",
    "# TODO: scale & crop before prediction!!!\n",
    "print(model_predict.predict(reshape_image(green)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
